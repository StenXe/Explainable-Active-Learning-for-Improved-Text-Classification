{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80afdef2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd8828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T10:53:59.138783Z",
     "start_time": "2021-12-04T10:53:55.863430Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346dec6",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9a3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T10:53:59.154828Z",
     "start_time": "2021-12-04T10:53:59.140812Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "max_features = 20000\n",
    "maxlen = 80                        # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "EPOCHS = 3\n",
    "class_sentiment_map = {0:'Negative', 1:'Positive'}\n",
    "q, c, p, b, k, initial_seed_size, initial_seed, j, S_indices, U_indices, train_indices, model_theta, num2word = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454e834",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebcabc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T10:54:00.789279Z",
     "start_time": "2021-12-04T10:53:59.156814Z"
    }
   },
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "def load_data():\n",
    "    print('Loading data...')\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, seed=RANDOM_SEED)\n",
    "    print(len(x_train), 'train sequences')\n",
    "    print(len(x_test), 'test sequences')\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def get_lstm_model():\n",
    "    print('Build model...')\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(Embedding(max_features, 128))\n",
    "    model_lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model_lstm.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model_lstm\n",
    "\n",
    "def init_workflow(x_train):\n",
    "    global q, c, p, b, k, initial_seed_size, initial_seed, j, S_indices, U_indices, model_theta, train_indices\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    p = 10\n",
    "    train_indices = [i for i in range(len(x_train))]\n",
    "    initial_seed_size = int(len(x_train) * 0.005)\n",
    "    b = initial_seed_size\n",
    "    k = 2 * initial_seed_size\n",
    "    initial_seed = np.random.choice(train_indices, size=initial_seed_size, replace=False)\n",
    "    j = 1\n",
    "    S_indices = initial_seed\n",
    "    U_indices = list(set(train_indices) - set(S_indices))\n",
    "\n",
    "def train_model_lstm(model_lstm, x_train, y_train):\n",
    "    print('Train...')\n",
    "    model_lstm.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=EPOCHS,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model_lstm.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(term_dict_1, term_dict_2):\n",
    "    cosine_sim = 0\n",
    "    for index in term_dict_1.keys():\n",
    "        if(index in term_dict_2):\n",
    "            cosine_sim += term_dict_1[index] * term_dict_2[index]\n",
    "    values_term_1 = sum([v**2 for k, v in term_dict_1.items()])\n",
    "    values_term_2 = sum([v**2 for k, v in term_dict_2.items()])\n",
    "\n",
    "    cosine_sim /= ( math.sqrt(values_term_1) * math.sqrt(values_term_2) )\n",
    "\n",
    "    return cosine_sim\n",
    "\n",
    "def compute_fidelity(model_theta, test_samples_i):\n",
    "    \n",
    "    x_subset = x_test[test_samples_i]\n",
    "    TOP_FEATURE_PCT = 0.1\n",
    "    TOP_FEATURES_LEN = int(len(x_subset[0]) * TOP_FEATURE_PCT)\n",
    "    \n",
    "    print(f'{len(x_subset)=} {TOP_FEATURES_LEN=}')\n",
    "    pred_proba_orig = model_theta.predict(x_subset)\n",
    "    pred_orig = np.argmax(pred_proba_orig, axis=1)\n",
    "    explainer = shap.DeepExplainer(model_theta, x_subset)\n",
    "    shap_values_labelled = explainer.shap_values(x_subset)\n",
    "    \n",
    "    pred_class_shap_values = []\n",
    "    x_new_subset = np.copy(x_subset)\n",
    "    for i in range(len(shap_values_labelled[0])):\n",
    "        \n",
    "        sort_index = np.argsort(-shap_values_labelled[pred_orig[i]][i])\n",
    "        top_features_index = sort_index[:TOP_FEATURES_LEN]\n",
    "        mask = np.ones(len(x_subset[0]), dtype=bool)\n",
    "        mask[top_features_index] = False\n",
    "        x_new_subset[i, mask] = 0\n",
    "\n",
    "    pred_proba_masked = model_theta.predict(x_new_subset)\n",
    "    pred_masked = np.argmax(pred_proba_masked, axis=1)\n",
    "    \n",
    "    fidelity = np.sum(pred_orig == pred_masked)\n",
    "    \n",
    "    avg_chg = np.mean((pred_proba_orig - pred_proba_masked), axis=0)\n",
    "    print(avg_chg)\n",
    "    \n",
    "    return fidelity\n",
    "\n",
    "def get_num2word():\n",
    "    global num2word\n",
    "    \n",
    "    if(not num2word):\n",
    "        offset = 3\n",
    "\n",
    "        words = imdb.get_word_index()\n",
    "        words = {k:(v+offset) for k,v in words.items()}\n",
    "        words[\"<PAD>\"] = 0\n",
    "        words[\"<START>\"] = 1\n",
    "        words[\"<UNK>\"] = 2\n",
    "        words[\"<UNUSED>\"] = 3\n",
    "\n",
    "        num2word = dict((i, word) for (word, i) in words.items())\n",
    "    \n",
    "    return num2word\n",
    "\n",
    "def shap_explanations(model_theta, x_test, y_test, sample_num):\n",
    "    explainer = shap.DeepExplainer(model_theta, x_test[:100])\n",
    "\n",
    "    shap_values = explainer.shap_values(np.array( [x_test[sample_num],] ))\n",
    "    shap.initjs()\n",
    "\n",
    "    num2word = get_num2word()\n",
    "    \n",
    "    x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[sample_num]))) for i in range(10)])\n",
    "\n",
    "    display(shap.force_plot(explainer.expected_value[y_test[sample_num]], shap_values[y_test[sample_num]][0], x_test_words[0]))\n",
    "    \n",
    "def decode_sentence(sample_num, x_test):\n",
    "    num2word = get_num2word()\n",
    "    decoded_sequence = \" \".join(num2word[i] for i in x_test[sample_num])\n",
    "    \n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed4ae8",
   "metadata": {},
   "source": [
    "### ALEX Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66994c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T21:45:54.322683Z",
     "start_time": "2021-12-01T12:47:50.281587Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "global q, c, p, b, k, initial_seed_size, initial_seed, j, S_indices, U_indices, model_theta, train_indices\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "init_workflow(x_train)\n",
    "\n",
    "while(j <= p):\n",
    "    print(f'==========Active Learning Iteration {j}/{p}==========')\n",
    "    x_train_subset = x_train[S_indices]\n",
    "    y_train_subset = y_train[S_indices]\n",
    "\n",
    "    print('Training Prediction Model')\n",
    "    model_theta = get_lstm_model()\n",
    "    model_theta = train_model_lstm(model_theta, x_train_subset, y_train_subset)\n",
    "\n",
    "    train_preds = np.argmax(model_theta.predict(x_train_subset), axis=1)\n",
    "\n",
    "    print('Generating Classification Report')\n",
    "    y_pred = np.argmax(model_theta.predict(x_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "    if(j == p):\n",
    "        break\n",
    "\n",
    "    print('Training Explainer Model on Labelled Set')\n",
    "    explainer = shap.DeepExplainer(model_theta, x_train_subset)\n",
    "\n",
    "    print(f'Computing Shap values for labelled instances - {len(x_train_subset)}')\n",
    "    shap_values_labelled = explainer.shap_values(x_train_subset)\n",
    "\n",
    "    print('Predicting unlabelled instances')\n",
    "    s_x = np.ndarray((len(U_indices), 3))\n",
    "    preds = model_theta.predict(x_train[U_indices])\n",
    "    # s_x[:, 0] = np.amax(preds, axis=1)\n",
    "    s_x[:, 0] = entropy(np.array(preds), base=2,axis=1)\n",
    "    s_x[:, 1] = U_indices\n",
    "    s_x[:, 2] = np.argmax(preds, axis=1)\n",
    "\n",
    "    sort_index = np.argsort(-s_x[:, 0])\n",
    "    s_x = s_x[sort_index]\n",
    "    c_indxs = list(map(int, s_x[:k, 1]))\n",
    "\n",
    "    pred_y_dict = dict(zip(s_x[:k, 1], s_x[:k, 2]))\n",
    "\n",
    "    print('Training Explainer Model on Unlabelled Candidate Set')\n",
    "    explainer = shap.DeepExplainer(model_theta, x_train[c_indxs])\n",
    "\n",
    "    print(f'Computing Shap values for unlabelled candidate set instances - {len(x_train[c_indxs])}')\n",
    "    shap_values_unlabelled = explainer.shap_values(x_train[c_indxs])\n",
    "\n",
    "    print('Computing Similarities')\n",
    "    xu_kld = np.ndarray((len(c_indxs), 2))\n",
    "    for i, index_xu in enumerate(tqdm(c_indxs)):\n",
    "        term_dict_U = dict(zip(x_train[int(index_xu)], shap_values_unlabelled[int(pred_y_dict[int(index_xu)])][i]))\n",
    "        for q, index_x in enumerate(S_indices):\n",
    "            term_dict_S = dict(zip(x_train[int(index_x)], shap_values_labelled[y_train[int(index_x)]][q]))\n",
    "            xu_kld[i][0] += compute_cosine_similarity(term_dict_S, term_dict_U)\n",
    "        xu_kld[i][0] /= len(S_indices)\n",
    "        xu_kld[i][1] = index_xu\n",
    "\n",
    "    sort_index = np.argsort(xu_kld[:, 0])\n",
    "    xu_kld = xu_kld[sort_index]                                             #least similar first; get such top k samples\n",
    "    delta_S = xu_kld[:b, 1]\n",
    "    print(f'Adding {len(delta_S)} samples to labelled set.')\n",
    "    S_indices = np.append(S_indices, list(map(int, delta_S)))\n",
    "    print(f'Total samples in labelled set {len(S_indices)}')\n",
    "    print(f'Removing {len(delta_S)} samples from unlabelled set')\n",
    "    U_indices = list(set(train_indices) - set(S_indices))\n",
    "    print(f'Total samples in unlabelled set {len(U_indices)}')\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db71c38",
   "metadata": {},
   "source": [
    "### ALEX Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396b8cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T22:12:14.396234Z",
     "start_time": "2021-12-03T21:45:54.370074Z"
    }
   },
   "outputs": [],
   "source": [
    "test_indices = [i for i in range(len(x_test))]\n",
    "\n",
    "global model_theta\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "test_samples_i = np.random.choice(test_indices, size=100, replace=False)\n",
    "fidelity = compute_fidelity(model_theta, test_samples_i)\n",
    "print(f'Fidelity {fidelity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e6d17",
   "metadata": {},
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f768872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T09:54:47.191634Z",
     "start_time": "2021-12-04T09:36:26.452106Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "global q, c, p, b, k, initial_seed_size, initial_seed, j, S_indices, U_indices, model_theta, train_indices\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "init_workflow(x_train)\n",
    "\n",
    "while(j <= p):\n",
    "    printmd(f'==========Active Learning Iteration {j}/{p}==========')\n",
    "    x_train_subset = x_train[S_indices]\n",
    "    y_train_subset = y_train[S_indices]\n",
    "    \n",
    "    print('Training Prediction Model')\n",
    "    model_theta = get_lstm_model()\n",
    "    model_theta = train_model_lstm(model_theta, x_train_subset, y_train_subset)\n",
    "    \n",
    "    if(j == p):\n",
    "        break\n",
    "    \n",
    "    print(f'Selecting random {initial_seed_size} samples')\n",
    "    delta_S = np.random.choice(U_indices, size=initial_seed_size, replace=False)\n",
    "    \n",
    "    print(f'Adding {len(delta_S)} samples to labelled set.')\n",
    "    S_indices = np.append(S_indices, list(map(int, delta_S)))\n",
    "    print(f'Total samples in labelled set {len(S_indices)}')\n",
    "    \n",
    "    print(f'Removing {len(delta_S)} samples from unlabelled set')\n",
    "    U_indices = list(set(train_indices) - set(S_indices))\n",
    "    print(f'Total samples in unlabelled set {len(U_indices)}')\n",
    "\n",
    "    U_indices = list(set(train_indices) - set(S_indices))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a6ced",
   "metadata": {},
   "source": [
    "### Random Sampling Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50177b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T16:04:24.582289Z",
     "start_time": "2021-11-29T16:02:25.055123Z"
    }
   },
   "outputs": [],
   "source": [
    "test_indices = [i for i in range(len(x_test))]\n",
    "\n",
    "global model_theta\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "test_samples_i = np.random.choice(test_indices, size=100, replace=False)\n",
    "fidelity = compute_fidelity(model_theta, test_samples_i)\n",
    "print(f'Fidelity {fidelity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef574ffe",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling Least Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80200ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T10:42:23.243775Z",
     "start_time": "2021-12-04T10:18:51.134868Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "global q, c, p, b, k, initial_seed_size, initial_seed, j, S_indices, U_indices, model_theta, train_indices\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "init_workflow(x_train)\n",
    "\n",
    "while(j <= p):\n",
    "    printmd(f'==========Active Learning Iteration {j}/{p}==========')\n",
    "    x_train_subset = x_train[S_indices]\n",
    "    y_train_subset = y_train[S_indices]\n",
    "    \n",
    "    print('Training Prediction Model')\n",
    "    model_theta = get_lstm_model()\n",
    "    model_theta = train_model_lstm(model_theta, x_train_subset, y_train_subset)\n",
    "    \n",
    "    if(j == p):\n",
    "        break\n",
    "    \n",
    "    print('Predicting unlabelled instances and computing uncertainty by least confidence')\n",
    "    s_x = np.ndarray((len(U_indices), 2))\n",
    "    s_x[:, 0] = 1-np.amax(model_theta.predict(x_train[U_indices]), axis=1)\n",
    "    s_x[:, 1] = U_indices\n",
    "    sort_index = np.argsort(s_x[:, 0])\n",
    "    s_x = s_x[-sort_index]   #larger the (1-model_top_pred) difference, lower the confidence of the model in that prediction, get such top k samples\n",
    "    delta_S = s_x[:k, 1]\n",
    "\n",
    "    print(f'Adding {len(delta_S)} samples to labelled set.')\n",
    "    S_indices = np.append(S_indices, list(map(int, delta_S)))\n",
    "    print(f'Total samples in labelled set {len(S_indices)}')\n",
    "    \n",
    "    print(f'Removing {len(delta_S)} samples from unlabelled set')\n",
    "    U_indices = list(set(train_indices) - set(S_indices))\n",
    "    print(f'Total samples in unlabelled set {len(U_indices)}')\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a5697",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling Least Confidence Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd99c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T16:41:38.028800Z",
     "start_time": "2021-11-29T16:39:37.765489Z"
    }
   },
   "outputs": [],
   "source": [
    "test_indices = [i for i in range(len(x_test))]\n",
    "\n",
    "global model_theta\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "test_samples_i = np.random.choice(test_indices, size=100, replace=False)\n",
    "fidelity = compute_fidelity(model_theta, test_samples_i)\n",
    "print(f'Fidelity {fidelity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82328df",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling Smallest Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53a842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T11:17:03.381871Z",
     "start_time": "2021-12-04T10:54:07.594332Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "global q, c, p, b, k, initial_seed_size, initial_seed, j, S_indices, U_indices, model_theta, train_indices\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "init_workflow(x_train)\n",
    "\n",
    "while(j <= p):\n",
    "    printmd(f'==========Active Learning Iteration {j}/{p}==========')\n",
    "    x_train_subset = x_train[S_indices]\n",
    "    y_train_subset = y_train[S_indices]\n",
    "    \n",
    "    print('Training Prediction Model')\n",
    "    model_theta = get_lstm_model()\n",
    "    model_theta = train_model_lstm(model_theta, x_train_subset, y_train_subset)\n",
    "    \n",
    "    if(j == p):\n",
    "        break\n",
    "    \n",
    "    print('Predicting unlabelled instances and computing uncertainty by smallest margin')\n",
    "    s_x = np.ndarray((len(U_indices), 2))\n",
    "    pred = model_theta.predict(x_train[U_indices])\n",
    "    s_x[:, 0] = abs(pred[:, 0] - pred[:, 1])\n",
    "    s_x[:, 1] = U_indices\n",
    "    sort_index = np.argsort(s_x[:, 0])\n",
    "    s_x = s_x[sort_index]   #smallest difference (margin) means model is struggling to differentiate between the two classes; take such top k samples\n",
    "    delta_S = s_x[:k, 1]\n",
    "\n",
    "    print(f'Adding {len(delta_S)} samples to labelled set.')\n",
    "    S_indices = np.append(S_indices, list(map(int, delta_S)))\n",
    "    print(f'Total samples in labelled set {len(S_indices)}')\n",
    "    \n",
    "    print(f'Removing {len(delta_S)} samples from unlabelled set')\n",
    "    U_indices = list(set(train_indices) - set(S_indices))\n",
    "    print(f'Total samples in unlabelled set {len(U_indices)}')\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9fd2a",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling Smallest Margin Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc4283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T18:28:19.088680Z",
     "start_time": "2021-11-29T18:26:29.327281Z"
    }
   },
   "outputs": [],
   "source": [
    "test_indices = [i for i in range(len(x_test))]\n",
    "\n",
    "global model_theta\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "test_samples_i = np.random.choice(test_indices, size=100, replace=False)\n",
    "fidelity = compute_fidelity(model_theta, test_samples_i)\n",
    "print(f'Fidelity {fidelity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab29b9",
   "metadata": {},
   "source": [
    "### SHAP Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e87210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T11:28:34.514652Z",
     "start_time": "2021-12-04T11:28:29.537591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global model_theta\n",
    "\n",
    "sample_num = 0            \n",
    "prediction = model_theta.predict(np.array([x_test[sample_num], ]))\n",
    "print(f'Review True Sentiment - {class_sentiment_map[y_test[sample_num]]}')\n",
    "print(f'Review Predicted Sentiment - {class_sentiment_map[np.argmax(prediction)]} with Probability - {np.max(prediction)}')\n",
    "shap_explanations(model_theta, x_test, y_test, sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e129c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T10:12:36.043892Z",
     "start_time": "2021-12-04T10:12:36.030894Z"
    }
   },
   "outputs": [],
   "source": [
    "print(decode_sentence(sample_num, x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
